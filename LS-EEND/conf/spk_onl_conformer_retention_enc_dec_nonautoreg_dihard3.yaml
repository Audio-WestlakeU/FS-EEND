log:
  model_name: spk_onl_tfm_enc_dec_10w_dihard3
  log_dir: !ref ./logs/<log[model_name]>
  save_top_k: -1
  start_epoch: 90
  end_epoch: 99
  save_avg_path: # !ref /mnt/home/liangdi/projects/pl_version/pl_eend/logs/spk_onl_tfm_enc_dec_10w/version_7/<model[arch]>_2spk_90_100_avg_model.ckpt

training:
  batch_size: 4
  n_workers: 8
  shuffle: True
  lr: 1.0e-05
  opt: adam                   # [adam, sgd, noam]
  max_epochs: 500
  grad_clip: 5
  grad_accm: 1
  scheduler:
  schedule_scale:  # 0.4 # 1.0
  warm_steps:  # 20000 # 100000
  early_stop_epoch: 500
  init_ckpt: logs/simu/8spk/onlineConformerDA_retention_emb1dcnn_linear_nonautoreg_l2norm_1-8spk_16_25_avg_model.ckpt # /mnt/home/liangdi/projects/pl_version/pl_eend/logs/spk_onl_tfm_enc_dec_10w/version_117/onlineConformerDA_retention_emb1dcnn_linear_nonautoreg_l2norm_allspk_41_50_avg_model.ckpt # ckpt path for model initiliazation
  dist_strategy: # ddp       # [ddp, dp]
  sync_batchnorm: True
  val_interval: 1             # validation after ? epoch(s) of training
  seed: 777


model:
  arch: onlineConformerDA_cummn_retention_emb1dcnn_linear_nonautoreg_l2norm_on_the_fly_lsa_padspk_before_loss2
  params:
    n_units: 256
    n_heads: 4
    enc_n_layers: 4
    feed_forward_expansion_factor: 4
    # dec_dim_feedforward: 2048
    conv_expansion_factor: 2
    dropout: 0.1
    conv_kernel_size: 16
    half_step_residual: True
    max_seqlen: !ref <data[chunk_size]>
    recurrent_chunk_size: 500
    dec_n_layers: 2
    conv_delay: 9
  
data:
  num_speakers:
  max_speakers:  
  context_recp: 7
  label_delay: 0  # number of frames delayed from original labels for uni-directional rnn to see in the future
  feat_type: logmel23_cummn # ['', 'log', 'logmel', 'logmel23', 'logmel23_mn', 'logmel23_mvn', 'logmel23_swn']
  chunk_size: 4000
  chunk_step: 4000
  val_chunk_size: 16000
  val_chunk_step: 4000
  subsampling: 10
  use_last_samples: True
  shuffle: False
  augment:
  feat:
    sample_rate: 8000
    win_length: 200
    n_fft: 1024
    hop_length: 80
    n_mels: 23
    f_max: 4000
    power: 1
  scaler:
    statistic: instance # instance or dataset-wide statistic
    normtype: minmax # minmax or standard or mean normalization
    dims: [1, 2] # dimensions over which normalization is applied

  train_data_dir: /data1/liangdi/dataset_spkdia/dihard3/data/dihard3_dev # swb_sre_tr_ns1n2n3n4n5n6_beta2n2n5n9n13n17_100000 # swb_sre_tr_ns1n2n3n4_beta2n2n5n9_100000 # swb_sre_tr_ns2_beta2_100000  # swb_sre_tr_ns1n2n3n4_beta2n2n2n2_20000
  val_data_dir: /data1/liangdi/dataset_spkdia/dihard3/data/dihard3_eval     # swb_sre_cv_ns1n2n3n4n5n6_beta2n2n5n9n13n17_500 # swb_sre_cv_ns1n2n3n4_beta2n2n5n9_500     # swb_sre_cv_ns2_beta2_500  # swb_sre_cv_ns1n2n3n4_beta2n2n2n2_500

task:
  max_speakers: 
  spk_attractor:
    enable: True
    shuffle: True
    enc_dropout: 0.5
    dec_dropout: 0.5
    consis_weight: 1  # 0 for not using  

# Used for debugging, how many data would be used in this run
debug:                
  num_sanity_val_steps: 3         # Validation steps before training
  log_every_n_steps: 5          # Frequency of updating logs
  # flush_logs_every_n_steps: 1     # Frequency of flushing logs
  # limit_train_batches: 0.1      # How many train data to be used (0-1)
  # limit_val_batches: 0.1          # How many val data to be used 
  # limit_test_batches: 0.1       # How many test data to be used 





# NOT SURE
# parser.add_argument('--gradient-accumulation-steps', default=1, type=int) |-> grad_acce

# parser.add_argument('--gpu', '-g', default=-1, type=int,
#                     help='GPU ID (negative value indicates CPU)')
# parser.add_argument('--num-frames', default=2000, type=int,
#                     help='number of frames in one utterance') |-> chunk_size
log:
  model_name: spk_STB
  log_dir: !ref ./logs/<log[model_name]>
  save_top_k: -1
  start_epoch: 40
  end_epoch: 49
  save_avg_path: # !ref <log[log_dir]>/version_39/<model[arch]>_2spk_90_100_avg_model.ckpt

training:
  batch_size: 1
  n_workers: 8
  shuffle: true
  lr: 1
  opt: noam                   # [adam, sgd, noam]
  max_epochs: 100
  grad_clip: 5
  grad_accm: 1
  scheduler: noam
  schedule_scale: 0.4
  warm_steps: 20000
  early_stop_epoch: 100
  init_ckpt: # logs/spk_dummy_nopad_10w/version_4/TransformerEDA_allspk_41_50_avg_model.ckpt # !ref <log[log_dir]>/version_30/<model[arch]>_2spk_90_100_avg_model.ckpt         # ckpt path for model initiliazation
  dist_strategy: # ddp_find_unused_parameters_false            # [ddp, dp]
  val_interval: 1             # validation after ? epoch(s) of training
  seed: 777

model:
  arch: TransformerEDA
  params:
    n_units: 256
    n_heads: 4
    n_layers: 4
    dropout: 0.1
    attractor_loss_ratio: 1.0
    attractor_encoder_dropout: 0.1
    attractor_decoder_dropout: 0.1


data:
  num_speakers:
  context_recp: 7
  label_delay: 0  # number of frames delayed from original labels for uni-directional rnn to see in the future
  feat_type: logmel23 # ['', 'log', 'logmel', 'logmel23', 'logmel23_mn', 'logmel23_mvn', 'logmel23_swn']
  chunk_size: 100000
  subsampling: 10
  use_last_samples: True
  shuffle: false
  buf_size: 1000 # 100s
  blk_size: 100    # 10s
  augment:
  feat:
    sample_rate: 8000
    win_length: 200
    n_fft: 1024
    hop_length: 80
    n_mels: 23
    f_max: 4000
    power: 1
  scaler:
    statistic: instance # instance or dataset-wide statistic
    normtype: minmax # minmax or standard or mean normalization
    dims: [1, 2] # dimensions over which normalization is applied

  train_data_dir: ./data/swb_sre_ts_ns4_beta9_500 
  val_data_dir: ./data/swb_sre_ts_ns4_beta9_500 
task:
  max_speakers: 4
  spk_attractor:
    enable: True
    shuffle: True
    enc_dropout: 0.5
    dec_dropout: 0.5
    consis_weight: 1  # 0 for not using  

# Used for debugging, how many data would be used in this run
debug:                
  num_sanity_val_steps: 3         # Validation steps before training
  log_every_n_steps: 100          # Frequency of updating logs
  # flush_logs_every_n_steps: 1     # Frequency of flushing logs
  # limit_train_batches: 0.1      # How many train data to be used (0-1)
  # limit_val_batches: 0.1          # How many val data to be used 
  # limit_test_batches: 0.1       # How many test data to be used 





# NOT SURE
# parser.add_argument('--gradient-accumulation-steps', default=1, type=int) |-> grad_acce

# parser.add_argument('--gpu', '-g', default=-1, type=int,
#                     help='GPU ID (negative value indicates CPU)')
# parser.add_argument('--num-frames', default=2000, type=int,
#                     help='number of frames in one utterance') |-> chunk_size